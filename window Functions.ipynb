{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8362e665-f385-4bec-a1c1-e22b136662c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Create a list of employee records\n",
    "employees = [\n",
    "    Row(empid=i, empname=f\"Employee_{i}\", deptNo=random.randint(1, 3), Salary=random.randint(100000, 400000))\n",
    "    for i in range(1, 11)\n",
    "]\n",
    "\n",
    "# Create a DataFrame\n",
    "employee_df = spark.createDataFrame(employees)\n",
    "\n",
    "# Create the employee table\n",
    "employee_df.write.saveAsTable(\"employee\", mode=\"overwrite\")\n",
    "\n",
    "# Display the employee table\n",
    "display(spark.sql(\"SELECT * FROM employee\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e560ee7-1186-4f7d-a140-1be00487311a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--SELECT * FROM employee where Salary = (select min(Salary) from employee);\n",
    "select *,(Salary-min_sal) sal_delta from (\n",
    "SELECT a.*,b.Salary as min_sal FROM employee a join (SELECT * FROM employee where Salary = (select min(Salary) from employee)) b on 1=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94555eb0-d034-4e6d-85d4-8d02b244c46c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "--SELECT *,min(salary) over ( order by Salary) as min_sal FROM employee\n",
    "SELECT *, min(Salary) OVER (partition by deptNo) as min_sal,((Salary - min(Salary) OVER (partition by deptNo))/Salary)*100 as percent_diff FROM employee order by deptNo asc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94b46e3a-2874-4a13-8d10-721bba768d03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT max(salary) FROM employee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cca0ed1-8928-4410-9ee3-1d8e70e2fe5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT *,max(salary) over () as max_sal FROM employee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec427147-5c60-47bb-84ec-831b21b0a2a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "employee_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cfb4f8f-9e0a-42c0-bbee-febd99765a1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import max,col\n",
    "\n",
    "window_spec = Window.partitionBy(col(\"deptNo\"))\n",
    "\n",
    "employee_df1 = employee_df.withColumn(\"Min_sal\",max(col(\"Salary\")).over(window_spec))\n",
    "employee_df1.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f29780a-c548-4148-b8ac-54b010e23d7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [(\"A\",1),(\"A\",6),(\"A\",10),(\"A\",4),(\"A\",5)]\n",
    "\n",
    "df = spark.createDataFrame(data,[\"id\",\"value\"])\n",
    "\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fbc0b44-85cb-4185-af4f-a87eb26a3bc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import sum,col,row_number\n",
    "\n",
    "window_spec = (\n",
    "    Window.partitionBy(\"id\")\n",
    "    .orderBy(\"value\")\n",
    "    .rowsBetween(Window.unboundedPreceding,0)\n",
    ")\n",
    "\n",
    "df.withColumn(\"sum\",sum(\"value\").over(window_spec)).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f97e2da4-7433-40f9-9d8c-5fd9e95f2914",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "window_spec = (\n",
    "    Window.partitionBy(\"id\")\n",
    "    .orderBy(\"value\")\n",
    "    .rowsBetween(0,Window.unboundedFollowing)\n",
    ")\n",
    "\n",
    "df.withColumn(\"sum\",sum(\"value\").over(window_spec)).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34e458f1-a882-42aa-a951-c5e6074af726",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "window_spec = (\n",
    "    Window.partitionBy(\"id\")\n",
    "    .orderBy(\"value\")\n",
    "    .rowsBetween(Window.unboundedPreceding,Window.unboundedFollowing)\n",
    ")\n",
    "\n",
    "df.withColumn(\"sum\",sum(\"value\").over(window_spec)).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01f080e0-2dae-4040-94be-b8f6310fa856",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "window_spec = (\n",
    "    Window.partitionBy(\"id\")\n",
    "    .orderBy(\"value\")\n",
    "    .rangeBetween(-1,5) # rangeBetween(4,10)\n",
    ")\n",
    "\n",
    "df.withColumn(\"sum\",sum(\"value\").over(window_spec)).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69e4fbbf-6c40-4d36-8ba7-0125e45b140b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "cummulative sum"
    }
   },
   "outputs": [],
   "source": [
    "data = [(2020,100),(2021,300),(2022,50),(2022,50),(2022,60),(2023,700),(2024,900)]\n",
    "\n",
    "df = spark.createDataFrame(data,[\"year\",\"revenue\"])\n",
    "\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf47d750-3e43-498f-8726-94d312db2c53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "window_spec = (\n",
    "    Window.orderBy(\"year\")\n",
    "    .rowsBetween(Window.unboundedPreceding,0)\n",
    ")\n",
    "\n",
    "df.withColumn(\"cummulative_sum\",sum(\"revenue\").over(window_spec)).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61c29164-68eb-46f1-a490-35cce3a345a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "window_spec = (\n",
    "    Window.orderBy(\"year\")\n",
    "    #.rowsBetween(Window.unboundedPreceding,0)\n",
    ")\n",
    "\n",
    "df.withColumn(\"cummulative_sum\",sum(\"revenue\").over(window_spec)).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bba3f9c-ada1-4567-bde0-91cdd06d38b2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "row_number()"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import sum,col,row_number,rank,dense_rank\n",
    "\n",
    "window_spec = (\n",
    "    Window.partitionBy(\"year\",\"revenue\")\n",
    "    .orderBy(\"revenue\")\n",
    ")\n",
    "\n",
    "df.withColumn(\"rn\",row_number().over(window_spec)).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51ee94eb-675f-4171-b93d-d63ac4350dad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_dedup = df.withColumn(\"rn\",row_number().over(window_spec)).filter(col(\"rn\") == 1)\n",
    "df_dedup.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "722b4fb4-f74e-4be8-943b-8dffd09ea506",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "rank"
    }
   },
   "outputs": [],
   "source": [
    "window_spec = (\n",
    "    Window\n",
    "    .orderBy(\"revenue\")\n",
    ")\n",
    "\n",
    "df.withColumn(\"rank\",rank().over(window_spec)).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "672b8099-fdb8-4714-b950-2c0f4537d837",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "dense_rank()"
    }
   },
   "outputs": [],
   "source": [
    "window_spec = (\n",
    "    Window\n",
    "    .orderBy(\"revenue\")\n",
    ")\n",
    "\n",
    "df.withColumn(\"dense_rank\",dense_rank().over(window_spec)).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23b08687-80b0-48a8-8e43-89b0818e476e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "rank() vs dense_rank() vs row_number()"
    }
   },
   "outputs": [],
   "source": [
    "window_spec = (\n",
    "    Window\n",
    "    .orderBy(\"revenue\")\n",
    ")\n",
    "\n",
    "df.withColumn(\"dense_rank\",dense_rank().over(window_spec)).withColumn(\"rank\",rank().over(window_spec)).withColumn(\"rowNumber\",row_number().over(window_spec)).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a7bbe57-c335-4d72-93cf-03438dea7276",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "3rd highest salary"
    }
   },
   "outputs": [],
   "source": [
    "window_spec = (\n",
    "    Window\n",
    "    .orderBy(col(\"Salary\").desc())\n",
    ")\n",
    "\n",
    "employee_df.withColumn(\"dense_rank\",dense_rank().over(window_spec)).filter(col(\"dense_rank\")==3).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1d19493-2b86-4270-90a6-80d3f919f46f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "window_spec = (\n",
    "    Window.partitionBy(\"deptNo\")\n",
    "    .orderBy(col(\"Salary\").desc())\n",
    ")\n",
    "\n",
    "employee_df.withColumn(\"dense_rank\",dense_rank().over(window_spec)).filter(col(\"dense_rank\") == 1).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00cecae3-94e7-4f5b-b716-88817ec2a865",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [(2020,100),(2021,300),(2022,50),(2022,50),(2022,60),(2023,700),(2024,900)]\n",
    "\n",
    "revenue_df = spark.createDataFrame(data,[\"year\",\"revenue\"])\n",
    "\n",
    "df.display()\n",
    "# 1.create a seperate column 'rev_diff' with revenue difference when compared to previous year\n",
    "# 2.create a seperate column 'P/F' based on 'rev_diff'  to know whether this year got Profit or loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7722d374-3c41-4839-b417-d4282653ed90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import ntile,lead,lag,first_value,last_value\n",
    "window_spec = (\n",
    "    Window.partitionBy(\"deptNo\")\n",
    "    .orderBy(col(\"Salary\").desc())\n",
    ")\n",
    "\n",
    "employee_df.withColumn(\"ntile_rank\",ntile(3).over(window_spec)).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6348acd-f0c4-4380-a708-336f9e7d5593",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##lead(column, offset, default): Value from a subsequent row.\n",
    "##lag(column, offset, default): Value from a preceding row.\n",
    "##first_value(column, ignoreNulls): First value in the window.\n",
    "##last_value(column, ignoreNulls): Last value in the window.\n",
    "\n",
    "data = [(2020,100),(2021,300),(2022,50),(2022,50),(2022,60),(2023,700),(2024,900)]\n",
    "\n",
    "revenue_df = spark.createDataFrame(data,[\"year\",\"revenue\"])\n",
    "\n",
    "revenue_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a94a52df-e986-4a28-9a92-c8d09baf6d25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import ntile,lead,lag,first_value,last_value,col,sum\n",
    "from pyspark.sql.window import Window\n",
    "window_spec = (\n",
    "    Window\n",
    "    .orderBy(col(\"year\").asc())\n",
    ")\n",
    "\n",
    "grouped_df = revenue_df.groupBy(\"year\").agg(sum(col(\"revenue\") ).alias(\"revenue\"))\n",
    "#grouped_df.display()\n",
    "grouped_df.withColumn(\"prev_revenue\", lag(col(\"revenue\"), 1, 0).over(window_spec)).withColumn(\"PnL\",col(\"revenue\")-col(\"prev_revenue\")).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e89a5bb-babf-4dca-a425-307821632940",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import ntile,lead,lag,first_value,last_value\n",
    "window_spec = (\n",
    "    Window\n",
    "    .orderBy(col(\"year\"))\n",
    ")\n",
    "\n",
    "grouped_df = revenue_df.groupBy(\"year\").agg(sum(col(\"revenue\") ).alias(\"revenue\"))\n",
    "#grouped_df.display()\n",
    "grouped_df.withColumn(\"prev_revenue\", lag(col(\"revenue\"), 1, 0).over(window_spec)).withColumn(\"next_revenue\", lead(col(\"revenue\"), 1, 0).over(window_spec)).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fe7f044-9955-4551-899a-1fd5a9e329fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import ntile,lead,lag,first_value,last_value,last\n",
    "window_spec = (\n",
    "    Window\n",
    "    .orderBy(col(\"year\").asc()).rowsBetween(Window.unboundedPreceding,Window.unboundedFollowing)\n",
    ")\n",
    "\n",
    "grouped_df = revenue_df.groupBy(\"year\").agg(sum(col(\"revenue\") ).alias(\"revenue\"))\n",
    "#grouped_df.display()\n",
    "result_df = grouped_df.withColumn(\"first_revenue\", first_value(col(\"revenue\")).over(window_spec)).withColumn(\"last_revenue\", last_value(col(\"revenue\")).over(window_spec))\n",
    "\n",
    "#result_df = grouped_df.withColumn(\"last_revenue\", last_value(col(\"revenue\"),True).over(window_spec))\n",
    "result_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3e54743-6fd4-4493-ba70-bc58ca6748e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [('2025-04-01',100,'Credit'),('2025-04-01',300,'Debit'),('2025-04-01',50,'Credit'),('2025-04-02',50,'Debit'),('2025-04-02',60,'Credit'),('2025-04-03',700,'Debit'),('2025-04-03',900,'Credit')]\n",
    "\n",
    "revenue_df = spark.createDataFrame(data,[\"date\",\"transaction\",\"trans_type\"])\n",
    "\n",
    "revenue_df.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "186409a4-35a9-4a4d-8ce6-5910dc9d18b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "data = [\n",
    "    Row(employee_id=1, name=\"John Doe\", department=\"HR\", salary=50000),\n",
    "    Row(employee_id=2, name=\"Jane Smith\", department=\"Finance\", salary=None),\n",
    "    Row(employee_id=3, name=None, department=\"IT\", salary=70000),\n",
    "    Row(employee_id=4, name=\"Mike Johnson\", department=None, salary=60000),\n",
    "    Row(employee_id=5, name=\"Emily Davis\", department=\"Marketing\", salary=None),\n",
    "    Row(employee_id=None, name=\"Anna Brown\", department=\"Sales\", salary=55000),\n",
    "    Row(employee_id=None, name=None, department=None, salary=None)\n",
    "]\n",
    "\n",
    "employee_df = spark.createDataFrame(data)\n",
    "\n",
    "display(employee_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "813a6ec7-3880-4df6-bcc2-eee614f69087",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#thresh considers only no of non null values\n",
    "employee_df.dropna(how ='any',thresh = 4).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a5acd86-b559-4798-9551-c89d421396e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "employee_df.dropna(how = 'all' ,subset = [\"employee_id\",\"name\"]).display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1627101835480962,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "window Functions",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
